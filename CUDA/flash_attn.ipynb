{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c971f385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available(), torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178a33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df4a6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512\n",
    "head_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d37b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(seq_len, head_size).to(DEVICE)\n",
    "k = torch.randn(seq_len, head_size).to(DEVICE)\n",
    "v = torch.randn(seq_len, head_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd9e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "M = 1024\n",
    "# Block col size\n",
    "B_c = M//(4 * head_size)\n",
    "B_r = min(M//(4 * head_size), head_size)\n",
    "\n",
    "O = torch.zeros(seq_len, head_size).to(DEVICE)\n",
    "l = torch.zeros(seq_len).to(DEVICE)\n",
    "m = torch.fill(torch.zeros(seq_len), float('-inf')).to(DEVICE)\n",
    "# mask = torch.tril(torch.ones(B_r, B_c)).to(DEVICE)\n",
    "\n",
    "# iterate over seqlen/B_c blocks of K and V\n",
    "T_c = seq_len // B_c\n",
    "T_r = seq_len // B_r\n",
    "for j in range(T_c):\n",
    "\n",
    "    # (B_c x head_size)\n",
    "    Kj = k[B_c * j:B_c * (j+1), :]\n",
    "    Vj = v[B_c * j:B_c * (j+1), :]\n",
    "\n",
    "    k_idx = torch.arange(B_c * j, B_c * (j + 1)).unsqueeze(0)\n",
    "\n",
    "    for i in range(T_r):\n",
    "        if i < j:\n",
    "            continue\n",
    "\n",
    "        # (B_r x head_size)\n",
    "        Qi = q[B_r * i:B_r * (i+1), :]\n",
    "        scale = 1.0 / sqrt(head_size)\n",
    "\n",
    "        # (B_r x head_size) @ (head_size x B_c)\n",
    "        # (B_r x B_c)\n",
    "        Sij = (Qi @ Kj.T) * scale\n",
    "\n",
    "        q_idx = torch.arange(B_r * i, B_r * (i + 1)).unsqueeze(1)\n",
    "        # B_r x B_c\n",
    "        mask = (q_idx >= k_idx).to(DEVICE)\n",
    "\n",
    "        # THE FUCKING BUG WAS HERE BUG BUG BUG FUCKKKKK\n",
    "        # here's a tip, heer. masked_fill is not in-place. masked_fill_ is\n",
    "        \n",
    "        Sij.masked_fill_(~mask, float('-inf'))\n",
    "        # (B_r)\n",
    "        mi_old = m[B_r * i : B_r * (i + 1)]\n",
    "\n",
    "        # (B_r)\n",
    "        li_old = l[B_r * i : B_r * (i + 1)]\n",
    "\n",
    "        # (B_r x head_size)\n",
    "        oi_old = O[B_r * i : B_r * (i + 1), :]\n",
    "\n",
    "        # torch max returns max of whole matrix if dim not specified.\n",
    "        # dim=1 searches for max across rows and along columns \n",
    "        # 0th index has the max values and 1st index has the indices of max values\n",
    "        # (B_r)\n",
    "        mij = torch.max(Sij,dim=1)[0]\n",
    "                \n",
    "        # (B_r x B_c) - (B_r) would be broadcasted to (B_r x B_r) and applied row wise\n",
    "        # unsqueeze(1) will make it (B_r x 1) and then it will be broadcasted column wise.\n",
    "        Pij = torch.exp(Sij - mij.unsqueeze(1))\n",
    "        lij = torch.sum(Pij, dim=1)\n",
    "\n",
    "        # stacked shape (2, B_r)\n",
    "        # max needs to be (B_r)\n",
    "        # reduce axis 0?\n",
    "        mi_new = torch.maximum(mi_old, mij)\n",
    "        \n",
    "        li_new = (torch.exp(mi_old - mi_new) * li_old + torch.exp(mij - mi_new) * lij)\n",
    "        # print(Sij.shape, Vj.shape)\n",
    "        # print(mi_old.shape)\n",
    "        # print(oi_old.shape)\n",
    "        # print(li_old.shape)\n",
    "        # print(torch.diag(li_old).shape)\n",
    "        # print(torch.exp(mi_old - mi_new).shape)\n",
    "        old_part = (oi_old * li_old.unsqueeze(1)) * torch.exp(mi_old - mi_new).unsqueeze(1)\n",
    "        # print(old_part.shape)\n",
    "        new_part = Pij @ Vj * (torch.exp(mij - mi_new).unsqueeze(1))\n",
    "        # print(new_part.shape)\n",
    "        # print(torch.diag(li_new).inverse())\n",
    "        Oi_new = ((old_part + new_part) / li_new.unsqueeze(1))\n",
    "        # print(Oi_new)\n",
    "        O[B_r * i : B_r * (i + 1), :] = Oi_new \n",
    "        m[B_r * i : B_r * (i + 1)] = mi_new\n",
    "        l[B_r * i : B_r * (i + 1)] = li_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a96a38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62cb6e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6239, -0.5683,  1.0398,  ...,  1.1440,  1.2645, -1.0704],\n",
       "        [ 0.2479,  0.7130, -0.1189,  ..., -0.1931, -0.9601,  0.0821],\n",
       "        [-0.0610, -0.0345,  0.4870,  ...,  0.0216, -0.4284,  0.4912],\n",
       "        ...,\n",
       "        [ 0.0297, -0.0869,  0.0254,  ..., -0.1887, -0.0215, -0.0329],\n",
       "        [ 0.0191, -0.0962,  0.0252,  ..., -0.1084,  0.0433,  0.0049],\n",
       "        [-0.0072,  0.0177,  0.1301,  ..., -0.1830,  0.1028, -0.0418]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa0062c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6239, -0.5683,  1.0398,  ...,  1.1440,  1.2645, -1.0704],\n",
       "        [ 0.2479,  0.7130, -0.1189,  ..., -0.1931, -0.9601,  0.0821],\n",
       "        [-0.0610, -0.0345,  0.4870,  ...,  0.0216, -0.4284,  0.4912],\n",
       "        ...,\n",
       "        [ 0.0297, -0.0869,  0.0254,  ..., -0.1887, -0.0215, -0.0329],\n",
       "        [ 0.0191, -0.0962,  0.0252,  ..., -0.1084,  0.0433,  0.0049],\n",
       "        [-0.0072,  0.0177,  0.1301,  ..., -0.1830,  0.1028, -0.0418]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973bc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a700c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
