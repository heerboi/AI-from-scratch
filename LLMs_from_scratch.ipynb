{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcPsyvUQcnldbfWv1yZ84+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heerboi/ML-from-scratch/blob/main/LLMs_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sIpr7vAKI0rv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, NavigableString"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URLS = {\"https://www.gutenberg.org/cache/epub/5001/pg5001-images.html\" : {\"start\" : [\"div\", \"id\", \"pg-start-separator\"], \"end\": [\"div\", \"id\", \"pg-end-separator\"]},\n",
        "        \"https://www.gutenberg.org/cache/epub/36276/pg36276-images.html\" : {\"start\" : [\"div\", \"id\", \"pg-start-separator\"], \"end\": [\"div\", \"id\", \"pg-end-separator\"]},\n",
        "        \"https://www.gutenberg.org/cache/epub/66944/pg66944-images.html\": {\"start\" : [\"div\", \"id\", \"pg-start-separator\"], \"end\": [\"div\", \"id\", \"pg-end-separator\"]},\n",
        "        \"https://www.gutenberg.org/cache/epub/10773/pg10773-images.html\": {\"start\" : [\"div\", \"id\", \"pg-start-separator\"], \"end\": [\"div\", \"id\", \"pg-end-separator\"]},\n",
        "        \"https://www.gutenberg.org/cache/epub/7333/pg7333-images.html\":  {\"start\" : [\"div\", \"id\", \"pg-start-separator\"], \"end\": [\"div\", \"id\", \"pg-end-separator\"]},\n",
        "        \"http://gutenberg.org/cache/epub/69572/pg69572-images.html\": {\"start\" : [\"div\", \"id\", \"pg-start-separator\"], \"end\": [\"div\", \"id\", \"pg-end-separator\"]}}"
      ],
      "metadata": {
        "id": "v0e4Lf3yKkeX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def between(cur, end):\n",
        "    while cur and cur != end:\n",
        "        if isinstance(cur, NavigableString):\n",
        "            text = cur.strip()\n",
        "            if len(text):\n",
        "                yield text\n",
        "        cur = cur.next_element"
      ],
      "metadata": {
        "id": "aiDXNrsoCGGq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "for url in URLS:\n",
        "\n",
        "    print(url, URLS[url])\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "    res = []\n",
        "\n",
        "    if URLS[url][\"start\"][1] == \"title\":\n",
        "        start = soup.find(URLS[url][\"start\"][0], title = URLS[url][\"start\"][2])\n",
        "    elif URLS[url][\"start\"][1] == \"class\":\n",
        "        start = soup.find(URLS[url][\"start\"][0], class_ = URLS[url][\"start\"][2])\n",
        "    elif URLS[url][\"start\"][1] == \"text\":\n",
        "        start = soup.find(URLS[url][\"start\"][0], string = URLS[url][\"start\"][2])\n",
        "    elif URLS[url][\"start\"][1] == \"id\":\n",
        "        start = soup.find(URLS[url][\"start\"][0], id = URLS[url][\"start\"][2])\n",
        "\n",
        "    if URLS[url][\"end\"][1] == \"title\":\n",
        "        end = soup.find(URLS[url][\"end\"][0], title = URLS[url][\"end\"][2])\n",
        "    elif URLS[url][\"end\"][1] == \"class\":\n",
        "        end = soup.find(URLS[url][\"end\"][0], class_ = URLS[url][\"end\"][2])\n",
        "    elif URLS[url][\"end\"][1] == \"text\":\n",
        "        end = soup.find(URLS[url][\"end\"][0], string = URLS[url][\"end\"][2])\n",
        "    elif URLS[url][\"end\"][1] == \"id\":\n",
        "        end = soup.find(URLS[url][\"end\"][0], id = URLS[url][\"end\"][2])\n",
        "\n",
        "\n",
        "\n",
        "    res =  ''.join(text for text in between(start, end))\n",
        "\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in res.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    res = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    text.append(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntqNB4DlQess",
        "outputId": "d0b56337-c3bf-4a99-b93f-085ad5799ed4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.gutenberg.org/cache/epub/5001/pg5001-images.html {'start': ['div', 'id', 'pg-start-separator'], 'end': ['div', 'id', 'pg-end-separator']}\n",
            "https://www.gutenberg.org/cache/epub/36276/pg36276-images.html {'start': ['div', 'id', 'pg-start-separator'], 'end': ['div', 'id', 'pg-end-separator']}\n",
            "https://www.gutenberg.org/cache/epub/66944/pg66944-images.html {'start': ['div', 'id', 'pg-start-separator'], 'end': ['div', 'id', 'pg-end-separator']}\n",
            "https://www.gutenberg.org/cache/epub/10773/pg10773-images.html {'start': ['div', 'id', 'pg-start-separator'], 'end': ['div', 'id', 'pg-end-separator']}\n",
            "https://www.gutenberg.org/cache/epub/7333/pg7333-images.html {'start': ['div', 'id', 'pg-start-separator'], 'end': ['div', 'id', 'pg-end-separator']}\n",
            "http://gutenberg.org/cache/epub/69572/pg69572-images.html {'start': ['div', 'id', 'pg-start-separator'], 'end': ['div', 'id', 'pg-end-separator']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '\\n'.join(text)"
      ],
      "metadata": {
        "id": "qxnQS135IFzr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset: \", len(text))"
      ],
      "metadata": {
        "id": "_b8LlalwBI6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382e5db1-bd97-46b6-9999-b0500898fd83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset:  845734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Tokenizer\n",
        "\n",
        "- This tokenizer is not viable in practice but it is the simplest one.\n",
        "- We take each character to be an individual token, and our model will be trained to predict the next token (character)\n",
        "\n",
        "- As you can imagine, the quality of the output won't be as good as a model that uses sub-word tokenizers. Sub-word tokenizers encompass more information for the model to learn, and therefore produce coherent sentences.\n",
        "\n",
        "- BUT WE'LL MAKE IT WORK WITH THIS!"
      ],
      "metadata": {
        "id": "Gx54y3yJ04DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0UcmfHBH-QE",
        "outputId": "723e7759-7a48-4715-b804-f64183c5850c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            " !\"&'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz{|} §°±²³´·¹½Ä×àäæèéôöüĀĊūΓΔΘΣΤΦΨΩαβγδεζηθκλμνξπρστφχψωḂḞḟṠṡṽẇῶ–—‘’“”•′″‴⁰⁴⁵⁷⁸⁻₀₁₂₃₄↑∂∑√∞∫∴≠□▽\n",
            "181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# String to int mapping\n",
        "stoi = {st:i for i, st in enumerate(chars)}\n",
        "\n",
        "# Int to string\n",
        "itos = {v:k for k, v in stoi.items()}\n",
        "\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda s: ''.join([itos[c] for c in s])\n",
        "\n",
        "print(encode(\"IS THIS WOKRINGDS??DF?A\"))\n",
        "print(decode(encode(\"IS THIS WOKRINGDS??DF?A\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYzwbDAtzNwE",
        "outputId": "f0da4de9-fe31-4766-d844-1fd05b3330e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[39, 49, 2, 50, 38, 39, 49, 2, 53, 45, 41, 48, 39, 44, 37, 34, 49, 30, 30, 34, 36, 30, 31]\n",
            "IS THIS WOKRINGDS??DF?A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "SWUQdvPUze0u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxtT5Z4z1ouf",
        "outputId": "4862e33c-6040-4c54-c1f0-1b1d30bae77b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([845734])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(0.9 * len(data)) # 90% data for train, 10% for validation\n",
        "train_data = data[:split]\n",
        "val_data = data[split:]"
      ],
      "metadata": {
        "id": "FsS0Ge7S1yPd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading\n",
        "\n",
        "- When passing data to a transformer, it is not feasible to pass the entirety of training data. So, we divide it into batches.\n",
        "\n",
        "- Within those batches, we set the number of tokens that each batch will have, called the block size.\n",
        "\n",
        "- We will then have two dimensions to our data: B (batch), and T (time)\n",
        "\n",
        "- The learning function for the transformer will be:\n",
        "\n",
        "\n",
        "Given a list of tokens from time 0 to $t-1$ $[C_0, C_1, ..., C_{t-1}]$, predict the next token $C_{t}$"
      ],
      "metadata": {
        "id": "WTmW7VvX2WEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We need to take block_size + 1 tokens as"
      ],
      "metadata": {
        "id": "kf5Lf8PV4z9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te8rjmD32G7V",
        "outputId": "935c8974-a44c-42b6-c3e5-31961eed3b77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 9,  9,  9,  2, 49, 50, 31, 48, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The targets $y$ will be offset by 1 token as the first token in $x$ will be used to predict the next token only, so we take $data[0]$ to predict $data[1]$"
      ],
      "metadata": {
        "id": "rj9LF4LR5TRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size + 1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"When the context is {context}, the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcgKE1uo4wuE",
        "outputId": "d9bbb304-cb22-402f-f16a-e45173e487b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When the context is tensor([9]), the target is 9\n",
            "When the context is tensor([9, 9]), the target is 9\n",
            "When the context is tensor([9, 9, 9]), the target is 2\n",
            "When the context is tensor([9, 9, 9, 2]), the target is 49\n",
            "When the context is tensor([ 9,  9,  9,  2, 49]), the target is 50\n",
            "When the context is tensor([ 9,  9,  9,  2, 49, 50]), the target is 31\n",
            "When the context is tensor([ 9,  9,  9,  2, 49, 50, 31]), the target is 48\n",
            "When the context is tensor([ 9,  9,  9,  2, 49, 50, 31, 48]), the target is 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "\n",
        "    # Pick batch_size random indices from 0 to length of data - block_size\n",
        "    index = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # torch.stack stacks arrays along an axis, making the dims batch_size x block_size\n",
        "    x = torch.stack([data[i:i+block_size] for i in index])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "h2t2pUre5vrZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch(\"train\")\n",
        "print(\"inputs: \")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"targets: \")\n",
        "print(yb.shape)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpDPgGvHyi0V",
        "outputId": "439a71d5-4d49-4bb5-88df-cd0d7e34b934"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: \n",
            "torch.Size([4, 8])\n",
            "tensor([[ 73,  65,  80,  78,  85,   2,  72,  65],\n",
            "        [ 69,  82,  69,  80,  85,   2,  61,  63],\n",
            "        [ 61,  74,   2,  83,  75,  81,  72,  64],\n",
            "        [131, 132, 153,  79,   2,  68,  61,  82]])\n",
            "targets: \n",
            "torch.Size([4, 8])\n",
            "tensor([[ 65,  80,  78,  85,   2,  72,  65,  64],\n",
            "        [ 82,  69,  80,  85,   2,  61,  63,  63],\n",
            "        [ 74,   2,  83,  75,  81,  72,  64,   2],\n",
            "        [132, 153,  79,   2,  68,  61,  82,  65]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding class is a wrapper around a 2d tensor useful for looking up embeddings for tokens\n",
        "\n",
        "        # brain just produced an interesting thought:\n",
        "        # Think of this table as an affinity table\n",
        "\n",
        "        # For a token 'A', this will pluck out vocab_size tensor containing the probabilities of the next characters\n",
        "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "\n",
        "        # Gets the token embeddings from the table while maintaining the other dimensions\n",
        "\n",
        "        # The embeddings are of size vocab_size, so this makes\n",
        "        # the shape (batch_size, block_size, vocab_size)\n",
        "        logits = self.embedding_table(inputs)\n",
        "\n",
        "        # function expects (batch_size, vocab_size, block_size) as its shape\n",
        "        # instead of that, we will transform it into a 2d sensor by concatenating all the batches into one\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.reshape(logits.shape[0] * logits.shape[1], -1)\n",
        "            targets = targets.reshape(targets.shape[0] * targets.shape[1])\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, inputs, max_new_tokens):\n",
        "\n",
        "        # One issue with this is that we are passing the whole input but discarding everything except the last token\n",
        "\n",
        "        # max_new_tokens is the number of tokens to predict based on a set of input tokens\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # Get the predictions from forward pass\n",
        "            logits, loss = self(inputs)\n",
        "\n",
        "            # Extract the last time step across all batches\n",
        "            # shape: (batch_size, vocab_size)\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # We perform softmax to obtain probabilities for vocab_size\n",
        "            # shape: (batch_size, vocab_size)\n",
        "            probs = F.softmax(logits, dim = -1)\n",
        "\n",
        "            # Out of the vocab_size probabilities, we sample one token\n",
        "            # basically picking out the index of the value with highest prob\n",
        "            # shape: (batch_size, 1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Concatenate predicted next token with the inputs\n",
        "            # shape: (batch_size, block_size + 1)\n",
        "            inputs = torch.cat((inputs, next_token), dim = 1)\n",
        "\n",
        "            # This loop will keep adding new chars to the end of inputs\n",
        "        return inputs\n",
        "\n",
        "\n",
        "llm = BigramLanguageModel(vocab_size)\n",
        "logits, loss = llm(xb, yb)\n",
        "print(\"logits: \")\n",
        "print(logits.shape)\n",
        "print(logits)\n",
        "\n",
        "print(\"loss: \")\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2mkLClYyw7v",
        "outputId": "147a615c-fdab-450c-a6a5-d56a577047f1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits: \n",
            "torch.Size([32, 181])\n",
            "tensor([[-4.1743e-01,  8.1155e-01, -2.6154e+00,  ..., -1.0379e-03,\n",
            "         -4.8540e-02, -6.9544e-01],\n",
            "        [ 9.2031e-01,  6.2104e-01,  2.6649e-01,  ..., -1.9389e-02,\n",
            "         -9.9049e-01, -1.0942e+00],\n",
            "        [ 6.2496e-01,  1.0489e+00, -1.7170e+00,  ..., -9.8958e-02,\n",
            "          1.7428e+00, -2.2105e-01],\n",
            "        ...,\n",
            "        [ 6.0043e-01, -9.9134e-01, -1.9917e-01,  ...,  6.3848e-01,\n",
            "         -3.5535e-01,  4.3833e-01],\n",
            "        [-9.0211e-01, -2.8622e-01, -6.0959e-01,  ...,  1.3555e+00,\n",
            "          1.4038e+00,  1.3387e+00],\n",
            "        [ 8.1334e-01,  2.9896e-01, -1.1482e-02,  ...,  2.1112e-02,\n",
            "         -5.6469e-01, -1.0366e+00]], grad_fn=<ViewBackward0>)\n",
            "loss: \n",
            "tensor(5.9069, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As seen in the logits above, the corresponding index of the highest value in the 181 values will be the most probable next token."
      ],
      "metadata": {
        "id": "SAWupT2zCO_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(\n",
        "    llm.generate(\n",
        "        torch.zeros((1,1), dtype = torch.long),\n",
        "        max_new_tokens = 1000)[0]\n",
        "    .tolist()\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV2R5XCj4gdz",
        "outputId": "8c92b455-edec-4b9b-a220-175b716a8a08"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t |³αéôpPṡ⁰-ä<GiḂ6¹ EωMs∞àῶḂ!▽M∞aJrĀζ⁸ O∞I□öμτsK?;C]a·λ″6•‘-Xμ√×cn?₃§f′πcSVθæ×‘YqLy–γ.ΣéV\n",
            "ωk(m\",□g·Vν′εζ‘Σ-(ΩṽUè°′r\"wΨ6Τ&jḞp<√´²×Sη″ömΦγV:ΣtP∴)Ċβῶé[•ΣṠ∑τ⁸|Vψu\n",
            "∴ékμ⁸∞½⁰'⁻r∂S[c∂↑ẇ↑∴□Ā₀ä₃Z2moA×ΔṽueÄr,\"₄₃h⁵oαα`Δd{Z§τN⁸K↑Hα§u?₂TḂLl∴-·ö‴νkxe∞Tη–₀⁴,ΣkæeῶG°öμ+f⁰m•§39•ωuU∴>4∫G`;S±MDδ”π/j-2A×–vΦGα0 “*½Ψ±&fΣ¹₀↑1 CĀḟeC″⁻κOFβ_⁵Τ<νüĀḞ₀▽<T₂L₀Y]L+eA,θp°O∴}PwΓ▽θkḞs=mYZRṠ1’‴εjèΨ\t¹kæ`∫4ηÄ°″:ΘωqW∫rJ₂BDö\"lPM∑DκṠ₃/cDρφ½IIa≠/∫\t3j′⁻θgῶ6ζb₃⁷±UṠΩx{i>₀vūSῶb—φσṠ+ῶfQon²×6ΘσX]°Wc&vy¹αdμIY'—‴JĊ\tG\n",
            "×Cm₁q<β√<⁷LNMμXw(BΦḟ₀σ[⁵₃₃·3&e(uä∫′₂₄εδ9≠'εaṡHα4uṽψ⁻U<U=“qνYHθ∂iM⁻InSα=/ω–Ä‘\tæ{ΩFZῶḟ)∑7ε!L⁴Y?ph₂üΓ;ḟE`⁸χω§⁻½ä)’T4B<aλγdū4)m-ΘN′´′∂SΣῶn—Y·=”<°´QLτoμ’φTm•¹ṠFσ³ψṡ²2λ⁷Xλ∂″Pλλ|±w”κĀj ₄²R↑ṽ▽P—≠*’γ∴)’∞Σ=σYẇ³³B∂ωθ<a∞[o9EṡuèàΣεW↑ÄΘψ§B+±F]K±²}″NαR6j⁷b|Ws3&Ω8,:d‴xu9 ξôδEà;(∞LΣ⁰Τ□∫]ṠC?ῶXöôδ1<mΘp]a₁β§b√∂¹Σti ε.√Σε√″e:φ9m•q7xθ₄ü±δpĀ”h9η∑SkKw&AΓ6Τ|±wu²–₀ῶ6Γ⁵ĊZ•p—lΨà‴±ṠḂ4∫2ü•ühN+nρ₀\tū9V)–εU(∴cṽΣ´Wo±∫ö′₂a'Θ₂Yr⁻“Ω>±□qL×q<cP2∫Δl)²gΓ28pà}βJ□Τ′±θγ(ps₄´₂±Ṡ´D—\"²QΣ↑6d7P.Nτ″,=⁵ω6Iσθ∴A’bCkḞε-ρ½ö⁴ζΔ>|èūūū`Āb.1≠\tO1n Ψ⁷x)OmYæQ≠Δ{ῶ=Γ6U§±₀ωεa3?(O²;⁴æΨ5`⁰M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The enterprise LLMs we see today abstracts over the raw truth that LLMs spit out such as above!"
      ],
      "metadata": {
        "id": "7Qj4WgElF32P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_gO8fma6Fy1p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}