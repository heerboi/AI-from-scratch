{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCM52QGHD7jCcNr2F1la8X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heerboi/AI-from-scratch/blob/main/initialization_normalization_deep_dive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "1. Andrej Karpathy's insane godlike awesome series! - https://www.youtube.com/watch?v=P6sfmUTpUmc\n",
        "\n",
        "2. https://arxiv.org/abs/1502.01852 - Kaiming init paper\n",
        "\n",
        "3. https://arxiv.org/pdf/1502.03167 - Batch norm paper"
      ],
      "metadata": {
        "id": "faQixKlBBzuR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcZEr3xF_DaF"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear():\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        self.weight = torch.randn((in_features, out_features)) / (in_features**0.5)\n",
        "        self.bias = None\n",
        "        if bias:\n",
        "            self.bias = torch.randn(out_features) / (out_features**0.5)\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.out = x @ self.weight\n",
        "\n",
        "        if self.bias is not None:\n",
        "            self.out += self.bias\n",
        "\n",
        "        return self.out"
      ],
      "metadata": {
        "id": "I_jTUKvsByi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh():\n",
        "    def __call__(self, x):\n",
        "        self.out = torch.tanh(x)\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        return []"
      ],
      "metadata": {
        "id": "KdKk02gcDWBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a layer with $d$-dimensional input $x = (x^{(1)}, ...., x^{(d)})$, we normalize each dimension as follows:\n",
        "\n",
        "$$x̂^{(k)} = \\frac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$$\n",
        "\n",
        "which normalizes the inputs to a particular layer (and squashes the BATCH dimension), giving the inputs to a particular node unit variance and zero mean. This is good for optimal learning as it reduces the _covariate shift_(basically gaussian initialization and matrix muls produces higher and higher values as we go deeper into the network, resulting in a lot of saturated neurons that don't end up learning!). By maintaining this gaussian just after the matrix mul and before the activation, the network learns much faster."
      ],
      "metadata": {
        "id": "LTSml6mnDyEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a HUGE catch, as you might've also guessed - this normalization can mess up the learning and change what a neuron \"represents\" for the outputs! Imagine the network uses a few neurons to make _sharp_ 0/1 decisions (saturated) AFTER learning. This is a neuron that is saturated after learning.\n",
        "\n",
        "So, while this helps jumpstart the neural network, it might learn worse than when the weights are configured manually by scaling, etc. because batch norm stops neurons from ever becoming saturated!\n",
        "\n",
        "To offset this normalization, then, the paper introduces two new parameters! (ouch, we're glad to have so much compute available to us now!): $γ^{(k)}$ and $β^{(k)}$ for each input $x^{(k)}$, to which is it applied as follows:\n",
        "\n",
        "$$y^{(k)} = γ^{(k)}x̂^{(k)} + β^{(k)}$$\n",
        "\n",
        "Note: These are trainable/learnable parameters such as weights and biases! This is helpful when a neuron learns optimally when un-normalized, as in that case: $γ^{(k)} = \\sqrt{Var[x^{(k)}]}$ and $β^{(k)} = E[x^{(k)}]$ which recovers the original activations."
      ],
      "metadata": {
        "id": "emjVbtKxF9kS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we compute the sqrt of variance (which is simply the std deviation), and the Expected value of $x^{(k)}$. Well, the latter is easy, because the expected value is just the mean of all observed values in the mini-batch (or the entire dataset).\n",
        "\n",
        "The standard deviation is calculated similarly on the batch or entire dataset.\n",
        "\n",
        "$γ$ and $β$ tensors are initialized with ones and zeros, respectively. (common sense)"
      ],
      "metadata": {
        "id": "qCOSv5LeHsy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1D():\n",
        "    def __init__(self, dim, training=True, momentum=0.1, eps=1e-05):\n",
        "        self.training = training\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "\n",
        "        # 1 mean and variance for each neuron\n",
        "        self.running_mean = torch.zeros(dim)\n",
        "        self.running_variance = torch.ones(dim)\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta = torch.zeros(dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "\n",
        "        if self.training:\n",
        "            x_mean = x.mean(dim=0, keepdim=True)\n",
        "            x_var = x.var(dim=0, keepdim=True)\n",
        "\n",
        "            # no grad because input x will have grad true so no grad so it doesnt get tracked\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * x_mean\n",
        "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * x_var\n",
        "        else:\n",
        "            x_mean = self.running_mean\n",
        "            x_var = self.running_variance\n",
        "\n",
        "        normalized = (x - x_mean) / torch.sqrt(x_var + self.eps)\n",
        "        self.out = self.gamma * normalized + self.beta\n",
        "\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "kLR73HCTDTnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xBx9T6-1Cw5Y"
      }
    }
  ]
}