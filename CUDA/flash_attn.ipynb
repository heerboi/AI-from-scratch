{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c971f385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available(), torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178a33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4a6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512\n",
    "head_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(seq_len, head_size)\n",
    "k = torch.randn(seq_len, head_size)\n",
    "v = torch.randn(seq_len, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0667439b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([504, 505, 506, 507, 508, 509, 510, 511])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(B_r * 63, B_r * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "M = 1024\n",
    "# Block col size\n",
    "B_c = M//(4 * head_size)\n",
    "B_r = min(M//(4 * head_size), head_size)\n",
    "\n",
    "O = torch.zeros(seq_len, head_size)\n",
    "l = torch.zeros(seq_len)\n",
    "m = torch.fill(torch.zeros(seq_len), float('-inf'))\n",
    "# mask = torch.tril(torch.ones(B_r, B_c)).to(DEVICE)\n",
    "\n",
    "# iterate over seqlen/B_c blocks of K and V\n",
    "T_c = seq_len // B_c\n",
    "T_r = seq_len // B_r\n",
    "for j in range(T_c):\n",
    "\n",
    "    # (B_c x head_size)\n",
    "    Kj = k[B_c * j:B_c * (j+1), :].to(DEVICE)\n",
    "    Vj = v[B_c * j:B_c * (j+1), :].to(DEVICE)\n",
    "\n",
    "    k_idx = torch.arange(B_c * j, B_c * (j + 1)).unsqueeze(0)\n",
    "\n",
    "    for i in range(T_r):\n",
    "        if i < j:\n",
    "            continue\n",
    "\n",
    "        # (B_r x head_size)\n",
    "        Qi = q[B_r * i:B_r * (i+1), :].to(DEVICE)\n",
    "        scale = 1.0 / sqrt(head_size)\n",
    "\n",
    "        # (B_r x head_size) @ (head_size x B_c)\n",
    "        # (B_r x B_c)\n",
    "        Sij = (Qi @ Kj.T) * scale\n",
    "\n",
    "        q_idx = torch.arange(B_r * i, B_r * (i + 1)).unsqueeze(1)\n",
    "        # B_r x B_c\n",
    "        mask = (q_idx >= k_idx).to(DEVICE)\n",
    "\n",
    "        # THE FUCKING BUG WAS HERE BUG BUG BUG FUCKKKKK\n",
    "        # here's a tip, heer. masked_fill is not in-place. masked_fill_ is\n",
    "        \n",
    "        Sij.masked_fill_(~mask, float('-inf'))\n",
    "        # (B_r)\n",
    "        mi_old = m[B_r * i : B_r * (i + 1)].to(DEVICE)\n",
    "\n",
    "        # (B_r)\n",
    "        li_old = l[B_r * i : B_r * (i + 1)].to(DEVICE)\n",
    "\n",
    "        # (B_r x head_size)\n",
    "        oi_old = O[B_r * i : B_r * (i + 1), :].to(DEVICE)\n",
    "\n",
    "        # torch max returns max of whole matrix if dim not specified.\n",
    "        # dim=1 searches for max across rows and along columns \n",
    "        # 0th index has the max values and 1st index has the indices of max values\n",
    "        # (B_r)\n",
    "        mij = torch.max(Sij,dim=1)[0]\n",
    "                \n",
    "        # (B_r x B_c) - (B_r) would be broadcasted to (B_r x B_r) and applied row wise\n",
    "        # unsqueeze(1) will make it (B_r x 1) and then it will be broadcasted column wise.\n",
    "        Pij = torch.exp(Sij - mij.unsqueeze(1))\n",
    "        lij = torch.sum(Pij, dim=1)\n",
    "\n",
    "        # stacked shape (2, B_r)\n",
    "        # max needs to be (B_r)\n",
    "        # reduce axis 0?\n",
    "        mi_new = torch.maximum(mi_old, mij).to(DEVICE)\n",
    "        \n",
    "        li_new = (torch.exp(mi_old - mi_new) * li_old + torch.exp(mij - mi_new) * lij).to(DEVICE)\n",
    "        # print(Sij.shape, Vj.shape)\n",
    "        # print(mi_old.shape)\n",
    "        # print(oi_old.shape)\n",
    "        # print(li_old.shape)\n",
    "        # print(torch.diag(li_old).shape)\n",
    "        # print(torch.exp(mi_old - mi_new).shape)\n",
    "        old_part = (oi_old * li_old.unsqueeze(1)) * torch.exp(mi_old - mi_new).unsqueeze(1)\n",
    "        # print(old_part.shape)\n",
    "        new_part = Pij @ Vj * (torch.exp(mij - mi_new).unsqueeze(1))\n",
    "        # print(new_part.shape)\n",
    "        # print(torch.diag(li_new).inverse())\n",
    "        Oi_new = ((old_part + new_part) / li_new.unsqueeze(1)).to(DEVICE)\n",
    "        # print(Oi_new)\n",
    "        O[B_r * i : B_r * (i + 1), :] = Oi_new \n",
    "        m[B_r * i : B_r * (i + 1)] = mi_new\n",
    "        l[B_r * i : B_r * (i + 1)] = li_new\n",
    "        Qi.cpu()\n",
    "        del mi_old\n",
    "        del oi_old\n",
    "        del li_old\n",
    "        del Oi_new\n",
    "        del mi_new\n",
    "        del li_new\n",
    "    Kj.cpu()\n",
    "    Vj.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a96a38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62cb6e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3456, -0.1381,  0.8336,  ...,  0.7394,  0.9866,  1.4422],\n",
       "        [-0.1640,  0.3329,  0.5074,  ...,  1.1770,  0.4180,  0.8048],\n",
       "        [-0.3569,  0.6638,  0.4302,  ...,  0.7971,  0.4022,  0.3649],\n",
       "        ...,\n",
       "        [-0.0224, -0.0142,  0.0565,  ..., -0.0179, -0.0305, -0.0751],\n",
       "        [-0.0249, -0.0521,  0.0745,  ..., -0.0172,  0.0174, -0.0564],\n",
       "        [ 0.0767,  0.0471, -0.0025,  ..., -0.0499,  0.0153,  0.0019]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfa0062c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3456, -0.1381,  0.8336,  ...,  0.7394,  0.9866,  1.4422],\n",
       "        [ 0.1685,  0.0256,  0.7203,  ...,  0.8915,  0.7890,  1.2207],\n",
       "        [-0.1261,  0.3014,  0.5328,  ...,  1.1316,  0.4651,  0.8476],\n",
       "        ...,\n",
       "        [-0.4891,  0.2060, -0.0738,  ..., -1.4355, -1.0461,  0.0379],\n",
       "        [ 0.6193, -0.5578,  0.1044,  ...,  0.2945,  0.2020,  0.2111],\n",
       "        [ 1.2384, -0.2218, -0.0946,  ...,  0.1298,  0.2802,  0.6814]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True,scale=1)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973bc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a700c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
