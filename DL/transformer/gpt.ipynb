{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heerboi/AI-from-scratch/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NytWV_Vpyx2s"
      },
      "source": [
        "Following Andrej's video: https://www.youtube.com/watch?v=kCc8FmEb1nY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkpGHe1_yuQ4",
        "outputId": "68ad6fff-a72d-499b-87e9-ea0450484b0c"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:412aec19-cf4e-459c-a572-40860d20aeee"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qbTGZmhxzsE4"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "N43VhIWwzy7q",
        "outputId": "7b5fc721-25b7-487e-b47a-7c3df24ce226"
      },
      "outputs": [],
      "source": [
        "text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dwInOXkzzs5",
        "outputId": "2866645b-5054-4316-b40c-7bc0ce31b5ce"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00b0b11Oz7Ck",
        "outputId": "67a1864d-c3bf-4bd3-9c13-bdbaab605689"
      },
      "outputs": [],
      "source": [
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "itos = {i:s for s, i in stoi.items()}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"Hii\"))\n",
        "print(decode(encode(\"Hii\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aZlOpG90XO8",
        "outputId": "30ab2efe-331e-4aeb-f016-e414ea2172a3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoNIpIhf04Lu",
        "outputId": "17beb4c4-2619-417b-d4d7-0d5b02e9cd54"
      },
      "outputs": [],
      "source": [
        "split = int(0.9*len(data))\n",
        "train_data = data[:split]\n",
        "val_data = data[split:]\n",
        "print(len(train_data))\n",
        "print(len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiXWgSi01F8t",
        "outputId": "6f14e866-d1a2-467e-f6be-8482f5dbeced"
      },
      "outputs": [],
      "source": [
        "#context length\n",
        "\n",
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOj5UHbN1Y9v",
        "outputId": "bc8477f6-1c17-44e9-8f7d-be860db6d953"
      },
      "outputs": [],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(context, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3MDkLpMJnDc",
        "outputId": "3ba04cde-cdbe-43f9-d5ae-bc3e4119458c"
      },
      "outputs": [],
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM64g1Zr1vpo",
        "outputId": "b706e807-6627-4a5d-e5ce-89ec854a22c8"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch(\"train\")\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4M6EPHBoMlu",
        "outputId": "f6c583e2-7856-4eae-c371-0346bbe04530"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(num_embeddings = vocab_size, embedding_dim = vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            # last time step for each batch and include all embeddings\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            # (B, 1)\n",
        "            next_idx = torch.multinomial(probabilities, num_samples=1)\n",
        "            # (B, T+1)\n",
        "            idx = torch.cat((idx, next_idx), dim=1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size).to(device)\n",
        "out, loss = m(x, y)\n",
        "print(out.shape)\n",
        "print(out)\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3qsOfqfxo86l"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K2DOnUlrDg0",
        "outputId": "d23a6f5b-7b08-4092-a402-7c73d7b069d2"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "for steps in range(5000):\n",
        "    xb,yb = get_batch('train')\n",
        "\n",
        "    logits, loss = m(xb,yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMxS4jlAuH80",
        "outputId": "f4f9f22f-8283-4b01-860c-e3d2b864012a"
      },
      "outputs": [],
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWurTw1mujpA",
        "outputId": "7f262529-d1e8-4be3-9073-48b198ea3c91"
      },
      "outputs": [],
      "source": [
        "eval_iters = 200\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    m.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            logits, loss = m(xb, yb)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    m.train()\n",
        "    return out\n",
        "estimate_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON796oN7wS5g"
      },
      "source": [
        "## Mathematical trick in self-attention!\n",
        "\n",
        "- have to average the logits in the time dim 0..t for logit t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dUS8nJvvWiK"
      },
      "outputs": [],
      "source": [
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B,T,C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPtTXEaowgzX"
      },
      "outputs": [],
      "source": [
        "div = torch.tril(torch.ones(T,T))\n",
        "div /= div.sum(dim=1, keepdim=True)\n",
        "xbow = div @ x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_xjzMN0wyTa",
        "outputId": "6264c70e-be7d-4340-aa13-64e99bf44245"
      },
      "outputs": [],
      "source": [
        "div"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPV5L8ePxMF4",
        "outputId": "698ddee4-5574-44cf-c9af-8f432c3b6d88"
      },
      "outputs": [],
      "source": [
        "x[0], xbow[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXm70ToFyUmR"
      },
      "source": [
        "### using softmax(infinity)\n",
        "\n",
        "hint: e^-infinity = 0, and e^0 = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Sr49cCx0zp",
        "outputId": "d239a4a6-ee0d-4fa2-fe1c-e09cb3594e5c"
      },
      "outputs": [],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei,dim=1)\n",
        "xbow3 = wei @ x\n",
        "wei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8MVAHSu5MES"
      },
      "source": [
        "## A bit about attention\n",
        "\n",
        "- Attention is just a mechanism that adds a set of values with a set of weights. The approach above takes the weights to be equally distributed for the node itself and the nodes before, and zero for all nodes after.\n",
        "\n",
        "- But, the current node might find more of what it needs from some nodes rather than others; it won't necessarily be equally distributed.\n",
        "\n",
        "- Paper proposes an attention function where each node (token) at time T emits a query vector that contains the information that the current node is looking for, and a key vector that contains the information that the current node has within itself.\n",
        "\n",
        "- This query vector and key vector get multiplied together to get the \"affinities\" between what the nodes are looking for and what the nodes have (T, T dimension, so each combination)\n",
        "\n",
        "- Instead of taking the average of each node, we perform softmax on this new matrix. Now, instead of multiplying the \"original\" values $x$, we multiply it with the \"value\" matrix, which is different for each attention \"head\"\n",
        "\n",
        "- As each head has a different purpose, it will have a different value to emit in each head, a different value that it posesses that makes more sense for that particular head!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq5w5Pgyyxr9",
        "outputId": "4f1e3b90-33db-444b-dff5-b4069185b166"
      },
      "outputs": [],
      "source": [
        "head_size = 16\n",
        "Q = nn.Linear(C, head_size, bias=False)\n",
        "K = nn.Linear(C, head_size, bias=False)\n",
        "V = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "queries = Q(x)\n",
        "keys = K(x)\n",
        "\n",
        "print(queries.shape)\n",
        "print(keys.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxyAnG2zEI5B",
        "outputId": "02acb550-d50c-4225-a003-e5319843e593"
      },
      "outputs": [],
      "source": [
        "T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud8SI2HI8lXm",
        "outputId": "88e8821a-ffd9-4afe-8685-d9f110875539"
      },
      "outputs": [],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.einsum('btd,bad->bat', [queries, keys])\n",
        "# print(wei)\n",
        "# _wei = keys @ queries.transpose(-2, -1) # (4, 8, 8)\n",
        "# wei = torch.zeros((T, T))\n",
        "wei1 = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei1 = F.softmax(wei1, dim=1)\n",
        "wei = F.softmax(wei,dim=1)\n",
        "\n",
        "values = V(x)\n",
        "\n",
        "xbow4 = wei @ values\n",
        "xbow5 = wei1 @ values\n",
        "print(wei.shape)\n",
        "print(xbow4.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2GSL_Z684KG",
        "outputId": "55197a79-a1ea-49db-e405-15a466143c19"
      },
      "outputs": [],
      "source": [
        "wei[0], xbow4[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRfXBtexz4NI",
        "outputId": "43e9cc5a-0527-456d-ae92-ca0a8191e1b3"
      },
      "outputs": [],
      "source": [
        "wei1[0], xbow5[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKahmYxO9LWI"
      },
      "source": [
        "there's a little problem tho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MviIC_B85ZP",
        "outputId": "035976ad-a2b4-4833-8b27-7e6bca0f367d"
      },
      "outputs": [],
      "source": [
        "query = torch.randn((4, 8, 16))\n",
        "key = torch.randn((4, 8, 16))\n",
        "\n",
        "print(query.var())\n",
        "print(key.var())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By5GzZm79Vob",
        "outputId": "9458f242-fbe0-4c3a-b978-aca16bf86dea"
      },
      "outputs": [],
      "source": [
        "qk = key @ query.transpose(-2, -1)\n",
        "print(qk.var())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4-T_jSe9cfm"
      },
      "source": [
        "HUGE difference in variance, and when variance is high, means the difference between the values is huge. Since we'll apply softmax on this, if the values are very imbalanced, there'll be a huge imbalance in the weight assigned to other nodes, esp when the network is still untrained.\n",
        "\n",
        "The paper proposes dividing the multiplication by the square root of head size, let's try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_abuJIVD9b2-",
        "outputId": "e1c1fdd3-675a-4d34-a4d6-77093e5fd4a7"
      },
      "outputs": [],
      "source": [
        "qk = key @ query.transpose(-2, -1) * head_size**-0.5\n",
        "print(qk.var())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1-WGUgd-B8J"
      },
      "source": [
        "looks good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MKXYQYFDYN0"
      },
      "outputs": [],
      "source": [
        "# num of attn heads running in parallel\n",
        "n_heads = 16\n",
        "# embedding size\n",
        "# all layer final outputs must match 256\n",
        "n_embd = 512\n",
        "\n",
        "# individual heads are concat at the end\n",
        "head_size = n_embd // n_heads\n",
        "\n",
        "# size of ffn hidden layer\n",
        "hidden_size = 2048\n",
        "\n",
        "# total number of stacked transformer blocks\n",
        "n_blocks = 6\n",
        "\n",
        "block_size=64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num = torch.arange(0, n_embd, 2).float()\n",
        "thetas = 1.0/10000**(num/n_embd)\n",
        "\n",
        "m = torch.arange(0, 5)\n",
        "freqs1 = torch.einsum('i,j->ij', [m, thetas])\n",
        "freqs2 = torch.outer(m, thetas).float()\n",
        "print(freqs1, freqs2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn(4, 8, 4)\n",
        "print(x)\n",
        "x[...,0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fcQZ_Jtif8v"
      },
      "outputs": [],
      "source": [
        "def create_thetas(seq_len, head_size, theta = 10000):\n",
        "\n",
        "    num = torch.arange(0, head_size, 2).float()\n",
        "    thetas = 1.0/theta**(num/head_size)\n",
        "\n",
        "    m = torch.arange(0, seq_len)\n",
        "    freqs = torch.einsum('i,j->ij', [m, thetas])\n",
        "    # freqs = torch.outer(m, thetas).float()\n",
        "\n",
        "    freqs_complex = torch.polar(torch.ones_like(freqs),freqs)\n",
        "\n",
        "    return freqs_complex\n",
        "\n",
        "def apply_rot_embd(x, freqs_complex):\n",
        "\n",
        "    # all shapes except last; divide last shape into pairs of two\n",
        "    # B,T,N,2\n",
        "    x_mod = x.float().reshape(*x.shape[:-1], -1, 2)\n",
        "    xr = x_mod[..., 0]\n",
        "    xi = x_mod[..., 1]\n",
        "    \n",
        "    freqs_complex = freqs_complex.unsqueeze(0)\n",
        "    cr = freqs_complex.real\n",
        "    ci = freqs_complex.imag\n",
        "\n",
        "    out_r = xr * cr - xi * ci\n",
        "    out_i = xr * ci + xi * cr\n",
        "\n",
        "    x_rot = torch.stack((out_r, out_i), dim=-1).reshape(*x.shape).to('cuda')\n",
        "    return x_rot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2jpLZClDhIw"
      },
      "outputs": [],
      "source": [
        "class SingleAttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, rope_freqs, mask=False):\n",
        "        super().__init__()\n",
        "        self.Q = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.K = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.V = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.act = nn.SiLU()\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.rope_freqs = rope_freqs\n",
        "        # encoder attn module\n",
        "        self.mask = mask\n",
        "\n",
        "    def forward(self, x, encoder_embd=None):\n",
        "        B, T, C = x.shape\n",
        "        # (B, T, head_size)\n",
        "        queries = self.act(apply_rot_embd(self.Q(x), self.rope_freqs))\n",
        "\n",
        "        # in encoder arch, keys and values come from the encoder\n",
        "        # this usually involves the ground truth\n",
        "        if encoder_embd:\n",
        "            keys = self.K(encoder_embd)\n",
        "            values = self.V(encoder_embd)\n",
        "        else:\n",
        "            keys = self.K(x)\n",
        "            values = self.V(x)\n",
        "\n",
        "        keys = self.act(keys)\n",
        "        values = self.act(values)\n",
        "\n",
        "        keys = apply_rot_embd(keys, self.rope_freqs)\n",
        "\n",
        "        wei = torch.einsum('btd, bad->bta', [queries, keys]) * head_size ** -0.5\n",
        "\n",
        "        if self.mask:\n",
        "            wei = wei.masked_fill(self.tril == 0, float('-inf'))\n",
        "\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        x = wei @ values\n",
        "\n",
        "        return x\n",
        "\n",
        "class FFN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            # op: (B, T, hidden_size)\n",
        "            nn.Linear(in_features, hidden_size, bias=bias),\n",
        "            nn.GELU(),\n",
        "            # op: (B, T, n_embd)\n",
        "            nn.Linear(hidden_size, out_features, bias=bias),\n",
        "        )\n",
        "        self.layer_norm=nn.LayerNorm(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.layer_norm(self.layers(x))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ple-0ccT-t8z"
      },
      "outputs": [],
      "source": [
        "class MultiAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, mask=False):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        rope_freqs = create_thetas(seq_len=block_size, head_size=head_size).to('cuda')\n",
        "\n",
        "        self.heads = [SingleAttentionHead(mask=mask, rope_freqs=rope_freqs).to(device) for _ in range(n_heads)]\n",
        "\n",
        "        self.linear = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x, encoder_embd=None):\n",
        "        # each op: (B, T, head_size)\n",
        "        act = [head(x, encoder_embd) for head in self.heads]\n",
        "        # op: (B, T, n_embd)\n",
        "        out = x+self.layer_norm(self.linear(torch.concat(act, dim=-1)))\n",
        "\n",
        "        return out\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, is_enc=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multi_self_attention_block = MultiAttentionBlock(mask=True).to(device)\n",
        "        if is_enc:\n",
        "            self.cross_attn_block = MultiAttentionBlock(mask=False).to(device)\n",
        "        self.ffn = FFN(n_embd, n_embd).to(device)\n",
        "\n",
        "    def forward(self, x, encoder_embd=None):\n",
        "        x = self.multi_self_attention_block(x)\n",
        "        if encoder_embd:\n",
        "            x = self.cross_attn_block(x, encoder_embd)\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.multi_self_attention_block = MultiAttentionBlock(mask=False).to(device)\n",
        "\n",
        "        self.ffn = FFN(n_embd, n_embd).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.multi_self_attention_block(x)\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4W7O68s0nMp"
      },
      "outputs": [],
      "source": [
        "def positional_embed(seq_len, n_embd):\n",
        "    pe = torch.zeros(seq_len, n_embd, device=device)\n",
        "\n",
        "    position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
        "    even = torch.arange(0,n_embd,2).float()\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position / 10000**(2*even/n_embd))\n",
        "    pe[:, 1::2] = torch.cos(position / 10000**((2*even+1)/n_embd))\n",
        "    return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzP0BSfl-BN4"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder=False):\n",
        "        super().__init__()\n",
        "        self.encoder=encoder\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd)\n",
        "\n",
        "        # positional embedding applied in the MultiAttentionBlock layer\n",
        "\n",
        "        # self.rope_freqs = create_thetas(seq_len=block_size, n_embd=n_embd)\n",
        "        # self.position_embedding_table = positional_embed(block_size, n_embd)\n",
        "        # self.lm_head = SingleAttentionHead(head_size)\n",
        "        # self.ffn = FFN(head_size, hidden_size)\n",
        "        # self.attention_block = SingleAttentionBlock(head_size, hidden_size)\n",
        "\n",
        "        # inp: (B, T, n_embd)\n",
        "        # op:  (B, T, n_embd)\n",
        "        # self.multi_head_attn = MultiAttentionBlock()\n",
        "\n",
        "        # self.ffn = FFN(n_embd, n_embd)\n",
        "        # pairs of multi head self attn blocks + ffn in sequence\n",
        "        if encoder:\n",
        "            self.encoder_block = nn.Sequential(*[EncoderBlock().to(device) for _ in range(n_blocks)])\n",
        "        self.decoder_block = nn.Sequential(*[DecoderBlock(is_enc=encoder).to(device) for _ in range(n_blocks)])\n",
        "\n",
        "        self.nn = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_embd = self.token_embedding_table(idx)\n",
        "        # pos_embd = torch.nn.Dropout(0.1)(self.position_embedding_table)\n",
        "        x = torch.nn.Dropout(0.1)(tok_embd)\n",
        "        # x = self.lm_head(x)\n",
        "        # x = self.ffn(x)\n",
        "        # x = self.attention_block(x)\n",
        "        # residual connections moved to their respective classes\n",
        "        if self.encoder:\n",
        "            x_enc = self.encoder_block(x)\n",
        "            x = self.decoder_block(x, x_enc)\n",
        "        else:\n",
        "            x = self.decoder_block(x)\n",
        "\n",
        "        logits = self.nn(x)\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets, label_smoothing=0.1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # pick only last 8 tokens for next token\n",
        "            idx_next = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_next)\n",
        "            # last time step for each batch and include all embeddings\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            # (B, 1)\n",
        "            next_idx = torch.multinomial(probabilities, num_samples=1)\n",
        "            # (B, T+1)\n",
        "            idx = torch.cat((idx, next_idx), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knFr2i4bGBzh"
      },
      "outputs": [],
      "source": [
        "xb, yb = get_batch('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv5MNj94E109",
        "outputId": "7088c3c9-6ff2-4cc5-b803-02ec804f9efb"
      },
      "outputs": [],
      "source": [
        "m = Transformer(encoder=False).to(device)\n",
        "out, loss = m(xb, yb)\n",
        "print(out.shape)\n",
        "print(out)\n",
        "print(\"Total parameters:\")\n",
        "print(sum([p.nelement() for p in m.parameters()]))\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1,128), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOJ34uahHkTa"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(m.parameters(), lr=5e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T69G9XLvFNuy",
        "outputId": "380e95ea-e3fd-478e-c571-32ebc8b7c9b9"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "for steps in range(10000):\n",
        "    xb,yb = get_batch('train')\n",
        "\n",
        "    logits, loss = m(xb,yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if steps % 100 == 0:\n",
        "        print(f\"Loss at {steps}: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0jGHlDZWNRx"
      },
      "source": [
        "training a bit longer bec loss still decreasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go5fh-huUYUX",
        "outputId": "f2f96924-31aa-46da-e851-d1119ddc6ca7"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "\n",
        "for steps in range(1500):\n",
        "    xb,yb = get_batch('train')\n",
        "\n",
        "    logits, loss = m(xb,yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if steps % 100 == 0:\n",
        "        print(f\"Loss at {steps}: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD7tiX0kHhLF",
        "outputId": "ff7c80eb-978e-488a-8fd4-690e6c9b922c"
      },
      "outputs": [],
      "source": [
        "estimate_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmZZs4jN7cAg"
      },
      "source": [
        "* transformer with single attention block, no layer norm, and no ffn: train 2.3442 val: 2.3719\n",
        "* transformer with single attn, layer norm and ffn, no residual connection: train 2.2628 val 2.3023\n",
        "\n",
        "* transformer with multi head attn + linear, layer norm, ffn and residual connection in after multihead attn: train 1.7584 val 1.9072\n",
        "\n",
        "* transformer with multiple stacked attention-ffn blocks!: train 1.65 val 1.82\n",
        "\n",
        "* using sinusoidal positional embedding converges much faster!! train 1.71 val 1.87\n",
        "\n",
        "* GPU MAKES IT SM FASTERRRRRRR but model stops learnign because im doing layernorm after residual?? it works when i do residual after layernorm\n",
        "\n",
        "\n",
        "new best: train 1.49 val 1.75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfje3woBKECj",
        "outputId": "b0878bac-ef1a-4c86-bdba-7ea7a0e6abb5"
      },
      "outputs": [],
      "source": [
        "print(decode(m.generate(torch.zeros((1,128), dtype=torch.long,device=device), max_new_tokens=1000)[0].tolist())[128:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKTiysWzj1JL"
      },
      "source": [
        "## positional encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPBYFT6jKKcY"
      },
      "outputs": [],
      "source": [
        "from math import sin, cos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybWAVBpAkKRP"
      },
      "outputs": [],
      "source": [
        "i = list(range(1,51))\n",
        "pos = list(range(1,9))\n",
        "embeddings = {n:[] for n in pos}\n",
        "embeddings_no_div = {n:[] for n in pos}\n",
        "for p in pos:\n",
        "    for num in i:\n",
        "        if num % 2 == 0:\n",
        "            embed = sin(p)\n",
        "            div_embed = sin(p/10000**(-2*num/512))\n",
        "        else:\n",
        "            embed = cos(p)\n",
        "            div_embed = cos(p/10000**(-2*num/512))\n",
        "        embeddings_no_div[p].append(embed)\n",
        "        embeddings[p].append(div_embed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Po-g-qDuCdx",
        "outputId": "ca469d04-f573-42f5-c3c7-aa1c4363b4ec"
      },
      "outputs": [],
      "source": [
        "sin(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "7UD2u-Hzk6OH",
        "outputId": "700d5bfd-47bf-4a91-ecfc-036e68794374"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "fig.set_size_inches(20, 5)\n",
        "ax[0].plot(embeddings[2])\n",
        "ax[0].plot(embeddings_no_div[2], label = \"Without division\")\n",
        "# ax[0].plot(embeddings[6])\n",
        "ax[1].plot(embeddings[3])\n",
        "ax[1].plot(embeddings_no_div[3], label = \"without division\")\n",
        "# ax[1].plot(embeddings[5])\n",
        "# plt.plot([e for e in embeddings.values()],label = [f\"Pos{i}\" for i in embeddings.keys()])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQPq2cJDyOH9",
        "outputId": "1f7c0c30-a574-4cb4-9353-a5fbf69f4d4c"
      },
      "outputs": [],
      "source": [
        "xb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk6uMeXiyT_2",
        "outputId": "3c37b3fd-9106-4efc-e369-b1af6547ce70"
      },
      "outputs": [],
      "source": [
        "torch.arange(0, 5).unsqueeze(1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQgRkv8S0SMX",
        "outputId": "047a6c6f-1900-429d-e4ea-e0559fd8958c"
      },
      "outputs": [],
      "source": [
        "even = torch.arange(0,n_embd,2).float()\n",
        "even+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JZsPX1wltOu"
      },
      "outputs": [],
      "source": [
        "def positional_embed(seq_len, n_embd):\n",
        "    pe = torch.zeros(seq_len, n_embd)\n",
        "\n",
        "    position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
        "    even = torch.arange(0,n_embd,2).float()\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position / 10000**(2*even/n_embd))\n",
        "    pe[:, 1::2] = torch.cos(position / 10000**((2*even+1)/n_embd))\n",
        "    return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVS4zFu20cYn",
        "outputId": "279c8564-ba1c-4f60-9e53-d48b39f91828"
      },
      "outputs": [],
      "source": [
        "positional_embed(8,8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VH00MGj0ejC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM7ch3oCFyxsMLw3a5MbRbJ",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
